{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# v3 Risk Estimator Notebook\n",
        "\n",
        "This notebook reproduces the v3 risk estimator workflow: labeling high-risk, training a classifier, calibration, and local explanations.\n",
        "\n",
        "Inputs:\n",
        "- `v3/data_clean/v3_features_v1.csv` or `v3/data_clean/v3_features_v2.csv`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Setup\n",
        "\n",
        "### Narrative commentary\n",
        "This notebook mirrors the v3 model logic in the dashboard, but keeps the workflow explicit for review.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, brier_score_loss\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "CWD = Path.cwd().resolve()\n",
        "if (CWD / \"v3\").exists():\n",
        "    REPO_ROOT = CWD\n",
        "elif CWD.name == \"notebooks\" and (CWD.parent / \"data_clean\").exists():\n",
        "    REPO_ROOT = CWD.parents[1]\n",
        "else:\n",
        "    REPO_ROOT = CWD\n",
        "\n",
        "V3_DIR = REPO_ROOT / \"v3\"\n",
        "DATA_CLEAN = V3_DIR / \"data_clean\"\n",
        "\n",
        "DATA_CLEAN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Load features\n",
        "\n",
        "### Narrative commentary\n",
        "Choose the source (v1 or v2). v1 is closer to real data; v2 is synthetic and larger.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SOURCE = \"v2\"  # change to \"v1\" if desired\n",
        "path = DATA_CLEAN / f\"v3_features_{SOURCE}.csv\"\n",
        "\n",
        "features = pd.read_csv(path)\n",
        "features.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) High-risk label\n",
        "\n",
        "### Narrative commentary\n",
        "High-risk is defined by a percentile cutoff on suicide_rate (default p80).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cutoff = 0.80\n",
        "threshold = features[\"suicide_rate\"].quantile(cutoff)\n",
        "features[\"high_risk\"] = (features[\"suicide_rate\"] >= threshold).astype(int)\n",
        "\n",
        "threshold, features[\"high_risk\"].mean()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Train a logistic model\n",
        "\n",
        "### Narrative commentary\n",
        "We use one-hot categorical features (region, income group, sex) and standardized numeric features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cat_cols = [\"region_name\", \"income_group\", \"sex_name\"]\n",
        "num_cols = [\"depression_dalys_rate\", \"addiction_death_rate\", \"selfharm_death_rate\"]\n",
        "\n",
        "X = features[cat_cols + num_cols]\n",
        "y = features[\"high_risk\"]\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    [\n",
        "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
        "        (\"num\", StandardScaler(), num_cols),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model = LogisticRegression(max_iter=1000, class_weight=\"balanced\")\n",
        "base = Pipeline([(\"prep\", preprocessor), (\"model\", model)])\n",
        "base.fit(X, y)\n",
        "\n",
        "calibrated = None\n",
        "if y.nunique() > 1 and len(features) >= 60:\n",
        "    calib_base = Pipeline([(\"prep\", preprocessor), (\"model\", model)])\n",
        "    try:\n",
        "        calibrated = CalibratedClassifierCV(estimator=calib_base, method=\"isotonic\", cv=3)\n",
        "    except TypeError:\n",
        "        calibrated = CalibratedClassifierCV(base_estimator=calib_base, method=\"isotonic\", cv=3)\n",
        "    calibrated.fit(X, y)\n",
        "\n",
        "predictor = calibrated if calibrated is not None else base\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Training diagnostics\n",
        "\n",
        "### Narrative commentary\n",
        "These metrics are training-only and serve as a quick sanity check.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "proba = predictor.predict_proba(X)[:, 1]\n",
        "preds = (proba >= 0.5).astype(int)\n",
        "\n",
        "metrics = {\n",
        "    \"accuracy\": accuracy_score(y, preds),\n",
        "    \"auc\": roc_auc_score(y, proba) if y.nunique() > 1 else np.nan,\n",
        "    \"brier\": brier_score_loss(y, proba),\n",
        "}\n",
        "metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Calibration plot\n",
        "\n",
        "### Narrative commentary\n",
        "If calibrated, predicted probabilities should align more closely with observed rates.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rel = pd.DataFrame({\"proba\": proba, \"y\": y})\n",
        "rel[\"bin\"] = pd.qcut(rel[\"proba\"], q=8, duplicates=\"drop\")\n",
        "cal = rel.groupby(\"bin\", observed=True).agg(mean_pred=(\"proba\", \"mean\"), observed_rate=(\"y\", \"mean\")).reset_index(drop=True)\n",
        "\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(x=cal[\"mean_pred\"], y=cal[\"observed_rate\"], mode=\"markers+lines\", name=\"Observed\"))\n",
        "fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode=\"lines\", name=\"Ideal\", line=dict(dash=\"dash\", color=\"#6b6460\")))\n",
        "fig.update_layout(title=\"Reliability plot\", xaxis_title=\"Mean predicted\", yaxis_title=\"Observed rate\")\n",
        "fig\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Counterfactual hints (10% reduction)\n",
        "\n",
        "### Narrative commentary\n",
        "This shows how the predicted probability changes if one feature is reduced by 10%.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "row = features.iloc[0].copy()\n",
        "base_input = pd.DataFrame([row])[cat_cols + num_cols]\n",
        "base_proba = float(predictor.predict_proba(base_input)[0][1])\n",
        "\n",
        "cf_rows = []\n",
        "for feature in num_cols:\n",
        "    new_row = row.copy()\n",
        "    new_row[feature] = max(0.0, float(new_row[feature]) * 0.9)\n",
        "    new_proba = float(predictor.predict_proba(pd.DataFrame([new_row])[cat_cols + num_cols])[0][1])\n",
        "    cf_rows.append({\"feature\": feature, \"delta_probability\": new_proba - base_proba})\n",
        "\n",
        "pd.DataFrame(cf_rows)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Local feature contributions\n",
        "\n",
        "### Narrative commentary\n",
        "For a single row, we approximate contributions using standardized inputs and logistic coefficients.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prep = base.named_steps[\"prep\"]\n",
        "clf = base.named_steps[\"model\"]\n",
        "feature_names = prep.get_feature_names_out()\n",
        "\n",
        "x_trans = prep.transform(base_input)\n",
        "if hasattr(x_trans, \"toarray\"):\n",
        "    x_vec = x_trans.toarray()[0]\n",
        "else:\n",
        "    x_vec = np.asarray(x_trans)[0]\n",
        "\n",
        "contrib = x_vec * clf.coef_.ravel()\n",
        "contrib_df = pd.DataFrame({\"feature\": feature_names, \"contribution\": contrib})\n",
        "contrib_df[\"abs\"] = contrib_df[\"contribution\"].abs()\n",
        "contrib_df = contrib_df.sort_values(\"abs\", ascending=False).head(10)\n",
        "\n",
        "fig = px.bar(\n",
        "    contrib_df.sort_values(\"contribution\"),\n",
        "    x=\"contribution\",\n",
        "    y=\"feature\",\n",
        "    orientation=\"h\",\n",
        "    title=\"Top local contributions\",\n",
        ")\n",
        "fig\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Notes\n",
        "\n",
        "- v3 is a synthetic risk estimator, not a clinical model.\n",
        "- Use these outputs to explain methodology, not individual outcomes.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}